{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14061624,"sourceType":"datasetVersion","datasetId":8950032},{"sourceId":14144201,"sourceType":"datasetVersion","datasetId":9014071}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"MobileNetV2 on CIFAR-10 dataset with pruning","metadata":{}},{"cell_type":"code","source":"import argparse\nimport os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom torchvision.models import mobilenet_v2, MobileNet_V2_Weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:07.052932Z","iopub.execute_input":"2025-12-13T15:26:07.053126Z","iopub.status.idle":"2025-12-13T15:26:14.302962Z","shell.execute_reply.started":"2025-12-13T15:26:07.053103Z","shell.execute_reply":"2025-12-13T15:26:14.302382Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ---------------------------------\n# Reproducibility Configuration\n# ---------------------------------\nimport torch\nimport numpy as np\nimport random\nimport os\n\ndef set_seed(seed=42):\n    \"\"\"\n    Sets seed for reproducibility across:\n    - Python\n    - NumPy\n    - PyTorch (CPU + GPU)\n    - cuDNN (deterministic)\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Make cuDNN deterministic (slower but reproducible)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Ensure hash-based ops are deterministic\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set to: {seed}\")\n\n# Call the seed function\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.304257Z","iopub.execute_input":"2025-12-13T15:26:14.304571Z","iopub.status.idle":"2025-12-13T15:26:14.317268Z","shell.execute_reply.started":"2025-12-13T15:26:14.304554Z","shell.execute_reply":"2025-12-13T15:26:14.316620Z"}},"outputs":[{"name":"stdout","text":"Random seed set to: 42\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def get_prunable_convs(model):\n    \"\"\"\n    Return an ordered list of (name, module) for Conv2d layers we want to prune.\n    Here: only pointwise 1x1 convs with groups = 1.\n    \"\"\"\n    prunable = []\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Conv2d) and m.kernel_size == (1, 1) and m.groups == 1:\n            prunable.append((name, m))\n    return prunable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.320472Z","iopub.execute_input":"2025-12-13T15:26:14.320706Z","iopub.status.idle":"2025-12-13T15:26:14.488939Z","shell.execute_reply.started":"2025-12-13T15:26:14.320689Z","shell.execute_reply":"2025-12-13T15:26:14.488249Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def channel_importance_l2(conv: nn.Conv2d):\n    \"\"\"\n    Compute L2 norm of each output channel in a Conv2d layer.\n    Returns a tensor of shape (out_channels,).\n    \"\"\"\n    W = conv.weight.detach()   # [out_c, in_c, k_h, k_w]\n    # Flatten spatial + input dims, then compute L2 per output channel\n    W_flat = W.view(W.size(0), -1)\n    importance = torch.norm(W_flat, p=2, dim=1)\n    return importance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.490591Z","iopub.execute_input":"2025-12-13T15:26:14.490894Z","iopub.status.idle":"2025-12-13T15:26:14.503901Z","shell.execute_reply.started":"2025-12-13T15:26:14.490865Z","shell.execute_reply":"2025-12-13T15:26:14.503212Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Subset\n\ndef make_calib_loader(test_dataset, batch_size=128, num_samples=1024):\n    indices = list(range(min(num_samples, len(test_dataset))))\n    calib_ds = Subset(test_dataset, indices)\n    calib_loader = torch.utils.data.DataLoader(\n        calib_ds, batch_size=batch_size, shuffle=False, num_workers=2\n    )\n    return calib_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.504717Z","iopub.execute_input":"2025-12-13T15:26:14.504971Z","iopub.status.idle":"2025-12-13T15:26:14.518810Z","shell.execute_reply.started":"2025-12-13T15:26:14.504949Z","shell.execute_reply":"2025-12-13T15:26:14.518293Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def count_params_per_layer(convs):\n    \"\"\"\n    convs: list of (name, conv_module)\n    Returns dict {name: num_params_in_layer}\n    \"\"\"\n    param_counts = {}\n    for name, conv in convs:\n        param_counts[name] = conv.weight.numel() + (conv.bias.numel() if conv.bias is not None else 0)\n    return param_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.519522Z","iopub.execute_input":"2025-12-13T15:26:14.520039Z","iopub.status.idle":"2025-12-13T15:26:14.536813Z","shell.execute_reply.started":"2025-12-13T15:26:14.520015Z","shell.execute_reply":"2025-12-13T15:26:14.536248Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def layer_cap(name: str) -> float:\n    \"\"\"\n    Per-layer max prune fraction based on MobileNetV2 block index.\n    \"\"\"\n    if not name.startswith(\"features.\"):\n        return 0.0\n\n    parts = name.split(\".\")\n    try:\n        block_idx = int(parts[1])\n    except (ValueError, IndexError):\n        return 0.0\n\n    # Final big conv: still conservative\n    if block_idx == 18:\n        return 0.20  # was 0.15\n\n    # Very early feature extractor: don't touch much\n    if block_idx <= 3:\n        return 0.10\n\n    # Early-mid (4–7): slightly higher\n    if block_idx <= 7:\n        return 0.20   # was 0.15\n\n    # Mid (8–13): can be more aggressive\n    if block_idx <= 13:\n        return 0.30   # was 0.20\n\n    # Late (14–17): even more room\n    if block_idx <= 17:\n        return 0.35   # was 0.25\n\n    return 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.537480Z","iopub.execute_input":"2025-12-13T15:26:14.538325Z","iopub.status.idle":"2025-12-13T15:26:14.551284Z","shell.execute_reply.started":"2025-12-13T15:26:14.538308Z","shell.execute_reply":"2025-12-13T15:26:14.550646Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_prunability_scores(sensitivities, eps=1e-3, min_sens=1e-3):\n    scores = {}\n    for name, s in sensitivities.items():\n        s_clamped = max(s, min_sens)   # avoid s=0\n        scores[name] = 1.0 / (eps + s_clamped)\n    return scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.552803Z","iopub.execute_input":"2025-12-13T15:26:14.553044Z","iopub.status.idle":"2025-12-13T15:26:14.565521Z","shell.execute_reply.started":"2025-12-13T15:26:14.553024Z","shell.execute_reply":"2025-12-13T15:26:14.565006Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def count_total_and_nonzero_params(model):\n    total = 0\n    nonzero = 0\n    for p in model.parameters():\n        if not p.requires_grad:\n            continue\n        numel = p.numel()\n        nz = (p != 0).sum().item()\n        total += numel\n        nonzero += nz\n    sparsity = 1.0 - (nonzero / total)\n    return total, nonzero, sparsity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.566389Z","iopub.execute_input":"2025-12-13T15:26:14.566695Z","iopub.status.idle":"2025-12-13T15:26:14.580428Z","shell.execute_reply.started":"2025-12-13T15:26:14.566663Z","shell.execute_reply":"2025-12-13T15:26:14.579787Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def allocate_prune_fractions(\n    scores,\n    param_counts,\n    target_global_prune=0.25,  # start more conservatively!\n):\n    names = list(scores.keys())\n    scores_vec = torch.tensor([scores[n] for n in names], dtype=torch.float32)\n    params_vec = torch.tensor([param_counts[n] for n in names], dtype=torch.float32)\n    caps_vec = torch.tensor([layer_cap(n) for n in names], dtype=torch.float32)\n\n    total_params = params_vec.sum().item()\n\n    if target_global_prune <= 0 or scores_vec.sum() == 0:\n        return {n: 0.0 for n in names}\n\n    def global_prune_for_alpha(alpha):\n        frac = torch.clamp(alpha * scores_vec, max=caps_vec)\n        pruned_params = (frac * params_vec).sum()\n        return (pruned_params / total_params).item()\n\n    low, high = 0.0, 1e6\n    for _ in range(50):\n        mid = 0.5 * (low + high)\n        g = global_prune_for_alpha(mid)\n        if g > target_global_prune:\n            high = mid\n        else:\n            low = mid\n\n    alpha_opt = low\n    frac_vec = torch.clamp(alpha_opt * scores_vec, max=caps_vec)\n\n    prune_fracs = {n: float(frac_vec[i].item()) for i, n in enumerate(names)}\n    return prune_fracs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.581143Z","iopub.execute_input":"2025-12-13T15:26:14.581325Z","iopub.status.idle":"2025-12-13T15:26:14.601232Z","shell.execute_reply.started":"2025-12-13T15:26:14.581310Z","shell.execute_reply":"2025-12-13T15:26:14.600641Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def build_optimizer_and_scheduler(model, base_lr, weight_decay, stage_epochs):\n    decay, no_decay = [], []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if param.ndimension() == 1 or name.endswith(\".bias\"):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n\n    optimizer = optim.SGD(\n        [\n            {\"params\": decay, \"weight_decay\": weight_decay},\n            {\"params\": no_decay, \"weight_decay\": 0.0},\n        ],\n        lr=base_lr,\n        momentum=0.9,\n        nesterov=True,\n    )\n\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=stage_epochs, eta_min=1e-4\n    )\n    return optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.601911Z","iopub.execute_input":"2025-12-13T15:26:14.602163Z","iopub.status.idle":"2025-12-13T15:26:14.616843Z","shell.execute_reply.started":"2025-12-13T15:26:14.602146Z","shell.execute_reply":"2025-12-13T15:26:14.616347Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def create_mobilenetv2_cifar10(num_classes=10, pretrained=True):\n    \"\"\"\n    Create a MobileNetV2 model adapted for CIFAR-10.\n    \"\"\"\n    if pretrained:\n        print(\"Load ImageNet weights\")\n        weights = MobileNet_V2_Weights.IMAGENET1K_V1\n        model = mobilenet_v2(weights=weights)\n    else:\n        model = mobilenet_v2(weights=None)\n\n    # Replace classifier (last linear layer) for CIFAR-10\n    in_features = model.classifier[1].in_features\n    model.classifier[1] = nn.Linear(in_features, num_classes)\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.617528Z","iopub.execute_input":"2025-12-13T15:26:14.617792Z","iopub.status.idle":"2025-12-13T15:26:14.636877Z","shell.execute_reply.started":"2025-12-13T15:26:14.617776Z","shell.execute_reply":"2025-12-13T15:26:14.636290Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_cifar10_loaders(data_dir, batch_size=128, num_workers=4):\n    \"\"\"\n    Returns (trainloader, testloader) for CIFAR-10 with good augmentations.\n    \"\"\"\n    # ImageNet-like normalization\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n\n    # CIFAR-10 images are 32x32; we upscale to 224x224 for MobileNetV2\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n        transforms.ToTensor(),\n        normalize,\n        transforms.RandomErasing(p=0.2)\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=train_transform\n    )\n\n    testset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=test_transform\n    )\n\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=batch_size, shuffle=True,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    return trainloader, testloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.639297Z","iopub.execute_input":"2025-12-13T15:26:14.639807Z","iopub.status.idle":"2025-12-13T15:26:14.652012Z","shell.execute_reply.started":"2025-12-13T15:26:14.639791Z","shell.execute_reply":"2025-12-13T15:26:14.651271Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class LabelSmoothingCrossEntropy(nn.Module):\n    \"\"\"\n    Cross-entropy with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, preds, target):\n        num_classes = preds.size(1)\n        log_preds = torch.log_softmax(preds, dim=1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_preds)\n            true_dist.fill_(self.smoothing / (num_classes - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n        return torch.mean(torch.sum(-true_dist * log_preds, dim=1))\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the top-k accuracy for the specified values of k.\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        # Get top-k indices\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        # Compare with targets expanded\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.652995Z","iopub.execute_input":"2025-12-13T15:26:14.653395Z","iopub.status.idle":"2025-12-13T15:26:14.671743Z","shell.execute_reply.started":"2025-12-13T15:26:14.653377Z","shell.execute_reply":"2025-12-13T15:26:14.671054Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_one_epoch(model, criterion, optimizer, dataloader, device, epoch, scaler=None):\n    model.train()\n    running_loss = 0.0\n    running_top1 = 0.0\n    total = 0\n\n    start_time = time.time()\n\n    for i, (inputs, targets) in enumerate(dataloader):\n        inputs = inputs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n\n        if scaler is not None:\n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n        top1, = accuracy(outputs, targets, topk=(1,))\n        bs = targets.size(0)\n        running_loss += loss.item() * bs\n        running_top1 += top1.item() * bs\n        total += bs\n\n        if (i + 1) % 100 == 0:\n            print(\n                f\"Epoch [{epoch}] Step [{i+1}/{len(dataloader)}] \"\n                f\"Loss: {running_loss / total:.4f} | \"\n                f\"Top-1: {running_top1 / total:.2f}%\"\n            )\n\n    epoch_loss = running_loss / total\n    epoch_acc1 = running_top1 / total\n    elapsed = time.time() - start_time\n    print(\n        f\"Epoch [{epoch}] TRAIN - \"\n        f\"Loss: {epoch_loss:.4f} | Top-1: {epoch_acc1:.2f}% | \"\n        f\"Time: {elapsed:.1f}s\"\n    )\n    return epoch_loss, epoch_acc1\n\n\n@torch.no_grad()\ndef evaluate(model, criterion, dataloader, device, epoch=\"TEST\"):\n    model.eval()\n    running_loss = 0.0\n    running_top1 = 0.0\n    total = 0\n\n    for inputs, targets in dataloader:\n        inputs = inputs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        top1, = accuracy(outputs, targets, topk=(1,))\n        bs = targets.size(0)\n        running_loss += loss.item() * bs\n        running_top1 += top1.item() * bs\n        total += bs\n\n    loss = running_loss / total\n    acc1 = running_top1 / total\n    print(f\"Epoch [{epoch}] VALID - Loss: {loss:.4f} | Top-1: {acc1:.2f}%\")\n    return loss, acc1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.672469Z","iopub.execute_input":"2025-12-13T15:26:14.672741Z","iopub.status.idle":"2025-12-13T15:26:14.692577Z","shell.execute_reply.started":"2025-12-13T15:26:14.672725Z","shell.execute_reply":"2025-12-13T15:26:14.692100Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def estimate_layer_sensitivity(model, criterion, device, calib_loader, base_prune=0.3):\n    \"\"\"\n    Estimate sensitivity of each prunable conv layer using your existing evaluate().\n    base_prune: fraction of channels to prune *only for sensitivity testing*.\n    Returns:\n        sensitivities: dict {layer_name: accuracy_drop (fraction, e.g. 0.01 = 1%)}\n        convs: ordered list of (name, conv_module)\n        baseline_acc: baseline accuracy on calib set (fraction)\n    \"\"\"\n    model.eval()\n    convs = get_prunable_convs(model)\n\n    # Baseline accuracy on calibration set\n    loss, baseline_acc = evaluate(model, criterion, calib_loader, device, epoch=\"CALIB_BASE\")\n    baseline_acc /= 100.0  # convert % -> fraction\n    print(f\"Baseline calib accuracy: {baseline_acc*100:.2f}%\")\n\n    sensitivities = OrderedDict()\n\n    for name, conv in convs:\n        print(f\"Testing sensitivity for layer: {name}\")\n        imp = channel_importance_l2(conv)\n        num_channels = imp.numel()\n        num_prune = int(base_prune * num_channels)\n        if num_prune < 1:\n            sensitivities[name] = 0.0\n            continue\n\n        prune_idxs = torch.argsort(imp)[:num_prune]\n\n        # Backup weights (and bias)\n        W_orig = conv.weight.data.clone()\n        b_orig = conv.bias.data.clone() if conv.bias is not None else None\n\n        # Temporary pruning\n        with torch.no_grad():\n            conv.weight.data[prune_idxs] = 0\n            if conv.bias is not None:\n                conv.bias.data[prune_idxs] = 0\n\n        # Evaluate with this layer pruned\n        _, acc1_pruned = evaluate(model, criterion, calib_loader, device, epoch=f\"CALIB_{name}\")\n        acc_pruned = acc1_pruned / 100.0\n        acc_drop = baseline_acc - acc_pruned\n        sensitivities[name] = float(acc_drop)\n        print(f\"  Acc with layer pruned: {acc_pruned*100:.2f}% (drop {acc_drop*100:.2f}%)\")\n\n        # Restore weights\n        with torch.no_grad():\n            conv.weight.data.copy_(W_orig)\n            if conv.bias is not None:\n                conv.bias.data.copy_(b_orig)\n\n    return sensitivities, convs, baseline_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.693286Z","iopub.execute_input":"2025-12-13T15:26:14.693508Z","iopub.status.idle":"2025-12-13T15:26:14.711043Z","shell.execute_reply.started":"2025-12-13T15:26:14.693485Z","shell.execute_reply":"2025-12-13T15:26:14.710460Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def apply_structured_channel_pruning(model, convs, prune_fracs):\n    \"\"\"\n    Zero entire output channels in selected Conv2d layers according to prune_fracs.\n    convs: list of (name, conv_module)\n    prune_fracs: dict {layer_name: fraction_to_prune}\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        for name, conv in convs:\n            p = prune_fracs.get(name, 0.0)\n            if p <= 0.0:\n                continue\n\n            imp = channel_importance_l2(conv)\n            num_channels = imp.numel()\n            num_prune = int(p * num_channels)\n            if num_prune < 1:\n                continue\n\n            prune_idxs = torch.argsort(imp)[:num_prune]\n\n            # Zero selected output channels\n            conv.weight.data[prune_idxs] = 0\n            if conv.bias is not None:\n                conv.bias.data[prune_idxs] = 0\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.711690Z","iopub.execute_input":"2025-12-13T15:26:14.711918Z","iopub.status.idle":"2025-12-13T15:26:14.729725Z","shell.execute_reply.started":"2025-12-13T15:26:14.711898Z","shell.execute_reply":"2025-12-13T15:26:14.729079Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Config:\n    data_dir = \"./data\"           # For Colab, use \"/content/data\"\n    epochs = 3\n    batch_size = 128\n    lr = 0.05\n    weight_decay = 4e-5\n    num_workers = 4\n    label_smoothing = 0.1\n    no_pretrained = False         # Set True to disable ImageNet pretraining\n    save_path = \"mobilenetv2_cifar10_best_pruned.pth\"\n    resume = \"\"                   # Path to checkpoint, or \"\" to start fresh\n    mixed_precision = False        # Use AMP if GPU is available\n    is_pruning = True             # Use this to detect pruning experiment is in progress\n\ncfg = Config()\nprint(\"Config:\", vars(cfg))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.730352Z","iopub.execute_input":"2025-12-13T15:26:14.730506Z","iopub.status.idle":"2025-12-13T15:26:14.746983Z","shell.execute_reply.started":"2025-12-13T15:26:14.730493Z","shell.execute_reply":"2025-12-13T15:26:14.746481Z"}},"outputs":[{"name":"stdout","text":"Config: {}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Data\ntrainloader, testloader = get_cifar10_loaders(\n    data_dir=cfg.data_dir,\n    batch_size=cfg.batch_size,\n    num_workers=cfg.num_workers,\n)\n\n# Model\nmodel = create_mobilenetv2_cifar10(\n    num_classes=10,\n    pretrained=not cfg.no_pretrained,\n)\nckpt = torch.load(\"/kaggle/input/fp32-trained-model/mobilenetv2_cifar10_best_baseline.pth\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\nmodel = model.to(device)\n\n# Dense model parameters for compression computation\ndense_total, dense_nonzero, dense_sparsity = count_total_and_nonzero_params(model)\nprint(f\"Dense model - total params: {dense_total}, nonzero: {dense_nonzero}, sparsity: {dense_sparsity*100:.2f}%\")\n\ncriterion = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\n\nval_loss, val_acc1 = evaluate(model, criterion, testloader, device, epoch=\"BASELINE\")\nprint(f\"Baseline accuracy: {val_acc1:.2f}%\")\n\n# Create a small calibration loader from your test dataset\ncalib_loader = make_calib_loader(testloader.dataset, batch_size=128, num_samples=1024)\n\n# Estimate layer sensitivities\nsensitivities, convs, baseline_acc = estimate_layer_sensitivity(\n    model,\n    criterion,\n    device,\n    calib_loader,\n    base_prune=0.2,\n)\n\n# Compute prunability scores\nscores = compute_prunability_scores(sensitivities)\nparam_counts = count_params_per_layer(convs)\n\n# Targets are \"global prune fractions\" on the prunable params\nstage_targets = [0.10, 0.15, 0.2, 0.25, 0.3]\nval_acc_threshold = 95.0  # stop if we go below this after fine-tune\nstage_epochs = 5          # fine-tune epochs per stage\nbase_lr_ft = 0.01\n\nbest_overall_acc = val_acc1\nbest_state_dict  = model.state_dict()\nbest_pruned_acc = 0.0\nbest_pruned_state = None\nbest_pruned_sparsity = 0.0\nbest_pruned_stage = None\nstage_results = []  # list of dicts: one entry per stage\n\n\nfor stage_idx, target in enumerate(stage_targets, 1):\n    print(\"=\" * 60)\n    print(f\"Stage {stage_idx}: target_global_prune = {target:.2f}\")\n    print(\"=\" * 60)\n\n    # 1) Allocate prune fractions for this target\n    prune_fracs = allocate_prune_fractions(\n        scores,\n        param_counts,\n        target_global_prune=target,\n    )\n\n    print(\"Per-layer prune fractions at this stage:\")\n    for name, frac in prune_fracs.items():\n        print(f\"{name:25s} -> {frac:.2f}\")\n\n    # 2) Apply pruning on top of current model\n    model = apply_structured_channel_pruning(model, convs, prune_fracs)\n\n    # 3) Evaluate immediately after pruning\n    _, acc_raw = evaluate(\n        model, criterion, testloader, device, epoch=f\"PRUNED_RAW_S{stage_idx}\"\n    )\n    print(f\"Stage {stage_idx} - accuracy after pruning, before fine-tune: {acc_raw:.2f}%\")\n\n    # 4) Measure sparsity & compression\n    total_params, nonzero_params, sparsity = count_total_and_nonzero_params(model)\n    compression_ratio = total_params / nonzero_params\n    print(\n        f\"Stage {stage_idx} - sparsity: {sparsity*100:.2f}% \"\n        f\"(compression {compression_ratio:.2f}x, \"\n        f\"nonzero {nonzero_params}/{total_params})\"\n    )\n\n    # 5) Short fine-tune for this stage\n    optimizer, scheduler = build_optimizer_and_scheduler(\n        model,\n        base_lr=base_lr_ft,\n        weight_decay=cfg.weight_decay,\n        stage_epochs=stage_epochs,\n    )\n\n    best_stage_acc = 0.0\n    for e in range(stage_epochs):\n        epoch_global = (stage_idx - 1) * stage_epochs + e\n        train_loss, train_acc = train_one_epoch(\n            model, criterion, optimizer, trainloader, device, epoch=epoch_global\n        )\n        val_loss, val_acc = evaluate(\n            model, criterion, testloader, device, epoch=f\"S{stage_idx}_E{e}\"\n        )\n        scheduler.step()\n\n        if val_acc > best_stage_acc:\n            best_stage_acc = val_acc\n\n    print(\n        f\"End of stage {stage_idx}: \"\n        f\"best_val_acc={best_stage_acc:.2f}%, \"\n        f\"sparsity={sparsity*100:.2f}%, \"\n        f\"compression={compression_ratio:.2f}x\"\n    )\n\n    # 6) Save this stage's model\n    stage_ckpt_path = f\"mobilenetv2_cifar10_pruned_stage{stage_idx}.pth\"\n    torch.save(\n        {\n            \"stage\": stage_idx,\n            \"target_global_prune\": target,\n            \"state_dict\": model.state_dict(),\n            \"best_val_acc\": best_stage_acc,\n            \"sparsity\": sparsity,\n            \"compression\": compression_ratio,\n            \"total_params\": total_params,\n            \"nonzero_params\": nonzero_params,\n        },\n        stage_ckpt_path,\n    )\n    print(f\"Saved stage {stage_idx} checkpoint to: {stage_ckpt_path}\")\n\n    # 7) Log metrics for later analysis\n    stage_results.append(\n        {\n            \"stage\": stage_idx,\n            \"target_global_prune\": target,\n            \"best_val_acc\": best_stage_acc,\n            \"acc_after_prune\": acc_raw,\n            \"sparsity\": sparsity,\n            \"compression\": compression_ratio,\n            \"ckpt_path\": stage_ckpt_path,\n        }\n    )\n\nprint(\"\\n=== Compression vs Accuracy per Stage ===\")\nfor r in stage_results:\n    print(\n        f\"Stage {r['stage']}: \"\n        f\"target={r['target_global_prune']:.2f}, \"\n        f\"sparsity={r['sparsity']*100:.2f}%, \"\n        f\"compression={r['compression']:.2f}x, \"\n        f\"acc_after_prune={r['acc_after_prune']:.2f}%, \"\n        f\"best_val_acc={r['best_val_acc']:.2f}%, \"\n        f\"ckpt={r['ckpt_path']}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:26:14.747713Z","iopub.execute_input":"2025-12-13T15:26:14.748090Z","iopub.status.idle":"2025-12-13T16:47:58.600492Z","shell.execute_reply.started":"2025-12-13T15:26:14.748069Z","shell.execute_reply":"2025-12-13T16:47:58.599636Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:03<00:00, 46.6MB/s] \nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","output_type":"stream"},{"name":"stdout","text":"Load ImageNet weights\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 118MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Dense model - total params: 2236682, nonzero: 2236682, sparsity: 0.00%\nEpoch [BASELINE] VALID - Loss: 0.6330 | Top-1: 96.24%\nBaseline accuracy: 96.24%\nEpoch [CALIB_BASE] VALID - Loss: 0.6367 | Top-1: 96.39%\nBaseline calib accuracy: 96.39%\nTesting sensitivity for layer: features.1.conv.1\nEpoch [CALIB_features.1.conv.1] VALID - Loss: 0.6934 | Top-1: 93.07%\n  Acc with layer pruned: 93.07% (drop 3.32%)\nTesting sensitivity for layer: features.2.conv.0.0\nEpoch [CALIB_features.2.conv.0.0] VALID - Loss: 0.6392 | Top-1: 96.19%\n  Acc with layer pruned: 96.19% (drop 0.20%)\nTesting sensitivity for layer: features.2.conv.2\nEpoch [CALIB_features.2.conv.2] VALID - Loss: 0.6462 | Top-1: 95.51%\n  Acc with layer pruned: 95.51% (drop 0.88%)\nTesting sensitivity for layer: features.3.conv.0.0\nEpoch [CALIB_features.3.conv.0.0] VALID - Loss: 0.6375 | Top-1: 96.00%\n  Acc with layer pruned: 96.00% (drop 0.39%)\nTesting sensitivity for layer: features.3.conv.2\nEpoch [CALIB_features.3.conv.2] VALID - Loss: 0.6358 | Top-1: 96.39%\n  Acc with layer pruned: 96.39% (drop 0.00%)\nTesting sensitivity for layer: features.4.conv.0.0\nEpoch [CALIB_features.4.conv.0.0] VALID - Loss: 0.6542 | Top-1: 94.92%\n  Acc with layer pruned: 94.92% (drop 1.46%)\nTesting sensitivity for layer: features.4.conv.2\nEpoch [CALIB_features.4.conv.2] VALID - Loss: 0.6672 | Top-1: 95.31%\n  Acc with layer pruned: 95.31% (drop 1.07%)\nTesting sensitivity for layer: features.5.conv.0.0\nEpoch [CALIB_features.5.conv.0.0] VALID - Loss: 0.6376 | Top-1: 95.80%\n  Acc with layer pruned: 95.80% (drop 0.59%)\nTesting sensitivity for layer: features.5.conv.2\nEpoch [CALIB_features.5.conv.2] VALID - Loss: 0.6450 | Top-1: 96.29%\n  Acc with layer pruned: 96.29% (drop 0.10%)\nTesting sensitivity for layer: features.6.conv.0.0\nEpoch [CALIB_features.6.conv.0.0] VALID - Loss: 0.6401 | Top-1: 96.00%\n  Acc with layer pruned: 96.00% (drop 0.39%)\nTesting sensitivity for layer: features.6.conv.2\nEpoch [CALIB_features.6.conv.2] VALID - Loss: 0.6433 | Top-1: 96.29%\n  Acc with layer pruned: 96.29% (drop 0.10%)\nTesting sensitivity for layer: features.7.conv.0.0\nEpoch [CALIB_features.7.conv.0.0] VALID - Loss: 0.6546 | Top-1: 94.82%\n  Acc with layer pruned: 94.82% (drop 1.56%)\nTesting sensitivity for layer: features.7.conv.2\nEpoch [CALIB_features.7.conv.2] VALID - Loss: 0.6587 | Top-1: 95.21%\n  Acc with layer pruned: 95.21% (drop 1.17%)\nTesting sensitivity for layer: features.8.conv.0.0\nEpoch [CALIB_features.8.conv.0.0] VALID - Loss: 0.6392 | Top-1: 96.19%\n  Acc with layer pruned: 96.19% (drop 0.20%)\nTesting sensitivity for layer: features.8.conv.2\nEpoch [CALIB_features.8.conv.2] VALID - Loss: 0.6460 | Top-1: 95.80%\n  Acc with layer pruned: 95.80% (drop 0.59%)\nTesting sensitivity for layer: features.9.conv.0.0\nEpoch [CALIB_features.9.conv.0.0] VALID - Loss: 0.6393 | Top-1: 96.19%\n  Acc with layer pruned: 96.19% (drop 0.20%)\nTesting sensitivity for layer: features.9.conv.2\nEpoch [CALIB_features.9.conv.2] VALID - Loss: 0.6421 | Top-1: 95.70%\n  Acc with layer pruned: 95.70% (drop 0.68%)\nTesting sensitivity for layer: features.10.conv.0.0\nEpoch [CALIB_features.10.conv.0.0] VALID - Loss: 0.6393 | Top-1: 96.00%\n  Acc with layer pruned: 96.00% (drop 0.39%)\nTesting sensitivity for layer: features.10.conv.2\nEpoch [CALIB_features.10.conv.2] VALID - Loss: 0.6379 | Top-1: 96.09%\n  Acc with layer pruned: 96.09% (drop 0.29%)\nTesting sensitivity for layer: features.11.conv.0.0\nEpoch [CALIB_features.11.conv.0.0] VALID - Loss: 0.6566 | Top-1: 95.41%\n  Acc with layer pruned: 95.41% (drop 0.98%)\nTesting sensitivity for layer: features.11.conv.2\nEpoch [CALIB_features.11.conv.2] VALID - Loss: 0.6628 | Top-1: 94.92%\n  Acc with layer pruned: 94.92% (drop 1.46%)\nTesting sensitivity for layer: features.12.conv.0.0\nEpoch [CALIB_features.12.conv.0.0] VALID - Loss: 0.6425 | Top-1: 96.09%\n  Acc with layer pruned: 96.09% (drop 0.29%)\nTesting sensitivity for layer: features.12.conv.2\nEpoch [CALIB_features.12.conv.2] VALID - Loss: 0.6484 | Top-1: 95.51%\n  Acc with layer pruned: 95.51% (drop 0.88%)\nTesting sensitivity for layer: features.13.conv.0.0\nEpoch [CALIB_features.13.conv.0.0] VALID - Loss: 0.6421 | Top-1: 95.61%\n  Acc with layer pruned: 95.61% (drop 0.78%)\nTesting sensitivity for layer: features.13.conv.2\nEpoch [CALIB_features.13.conv.2] VALID - Loss: 0.6433 | Top-1: 95.90%\n  Acc with layer pruned: 95.90% (drop 0.49%)\nTesting sensitivity for layer: features.14.conv.0.0\nEpoch [CALIB_features.14.conv.0.0] VALID - Loss: 0.6462 | Top-1: 95.80%\n  Acc with layer pruned: 95.80% (drop 0.59%)\nTesting sensitivity for layer: features.14.conv.2\nEpoch [CALIB_features.14.conv.2] VALID - Loss: 0.6659 | Top-1: 94.82%\n  Acc with layer pruned: 94.82% (drop 1.56%)\nTesting sensitivity for layer: features.15.conv.0.0\nEpoch [CALIB_features.15.conv.0.0] VALID - Loss: 0.6564 | Top-1: 95.12%\n  Acc with layer pruned: 95.12% (drop 1.27%)\nTesting sensitivity for layer: features.15.conv.2\nEpoch [CALIB_features.15.conv.2] VALID - Loss: 0.6517 | Top-1: 95.80%\n  Acc with layer pruned: 95.80% (drop 0.59%)\nTesting sensitivity for layer: features.16.conv.0.0\nEpoch [CALIB_features.16.conv.0.0] VALID - Loss: 0.6420 | Top-1: 96.00%\n  Acc with layer pruned: 96.00% (drop 0.39%)\nTesting sensitivity for layer: features.16.conv.2\nEpoch [CALIB_features.16.conv.2] VALID - Loss: 0.6449 | Top-1: 95.70%\n  Acc with layer pruned: 95.70% (drop 0.68%)\nTesting sensitivity for layer: features.17.conv.0.0\nEpoch [CALIB_features.17.conv.0.0] VALID - Loss: 0.8912 | Top-1: 96.19%\n  Acc with layer pruned: 96.19% (drop 0.20%)\nTesting sensitivity for layer: features.17.conv.2\nEpoch [CALIB_features.17.conv.2] VALID - Loss: 0.6958 | Top-1: 96.48%\n  Acc with layer pruned: 96.48% (drop -0.10%)\nTesting sensitivity for layer: features.18.0\nEpoch [CALIB_features.18.0] VALID - Loss: 0.6576 | Top-1: 96.19%\n  Acc with layer pruned: 96.19% (drop 0.20%)\n============================================================\nStage 1: target_global_prune = 0.10\n============================================================\nPer-layer prune fractions at this stage:\nfeatures.1.conv.1         -> 0.01\nfeatures.2.conv.0.0       -> 0.10\nfeatures.2.conv.2         -> 0.04\nfeatures.3.conv.0.0       -> 0.08\nfeatures.3.conv.2         -> 0.10\nfeatures.4.conv.0.0       -> 0.03\nfeatures.4.conv.2         -> 0.03\nfeatures.5.conv.0.0       -> 0.06\nfeatures.5.conv.2         -> 0.20\nfeatures.6.conv.0.0       -> 0.08\nfeatures.6.conv.2         -> 0.20\nfeatures.7.conv.0.0       -> 0.02\nfeatures.7.conv.2         -> 0.03\nfeatures.8.conv.0.0       -> 0.14\nfeatures.8.conv.2         -> 0.06\nfeatures.9.conv.0.0       -> 0.14\nfeatures.9.conv.2         -> 0.05\nfeatures.10.conv.0.0      -> 0.08\nfeatures.10.conv.2        -> 0.10\nfeatures.11.conv.0.0      -> 0.04\nfeatures.11.conv.2        -> 0.03\nfeatures.12.conv.0.0      -> 0.10\nfeatures.12.conv.2        -> 0.04\nfeatures.13.conv.0.0      -> 0.05\nfeatures.13.conv.2        -> 0.07\nfeatures.14.conv.0.0      -> 0.06\nfeatures.14.conv.2        -> 0.02\nfeatures.15.conv.0.0      -> 0.03\nfeatures.15.conv.2        -> 0.06\nfeatures.16.conv.0.0      -> 0.08\nfeatures.16.conv.2        -> 0.05\nfeatures.17.conv.0.0      -> 0.14\nfeatures.17.conv.2        -> 0.20\nfeatures.18.0             -> 0.14\nEpoch [PRUNED_RAW_S1] VALID - Loss: 0.7603 | Top-1: 91.98%\nStage 1 - accuracy after pruning, before fine-tune: 91.98%\nStage 1 - sparsity: 9.32% (compression 1.10x, nonzero 2028186/2236682)\nEpoch [0] Step [100/391] Loss: 0.7157 | Top-1: 93.23%\nEpoch [0] Step [200/391] Loss: 0.7082 | Top-1: 93.38%\nEpoch [0] Step [300/391] Loss: 0.7083 | Top-1: 93.29%\nEpoch [0] TRAIN - Loss: 0.7067 | Top-1: 93.31% | Time: 176.2s\nEpoch [S1_E0] VALID - Loss: 0.6514 | Top-1: 95.25%\nEpoch [1] Step [100/391] Loss: 0.6872 | Top-1: 93.98%\nEpoch [1] Step [200/391] Loss: 0.6940 | Top-1: 93.56%\nEpoch [1] Step [300/391] Loss: 0.6940 | Top-1: 93.66%\nEpoch [1] TRAIN - Loss: 0.6938 | Top-1: 93.71% | Time: 181.3s\nEpoch [S1_E1] VALID - Loss: 0.6499 | Top-1: 95.42%\nEpoch [2] Step [100/391] Loss: 0.6819 | Top-1: 94.15%\nEpoch [2] Step [200/391] Loss: 0.6830 | Top-1: 94.19%\nEpoch [2] Step [300/391] Loss: 0.6822 | Top-1: 94.27%\nEpoch [2] TRAIN - Loss: 0.6833 | Top-1: 94.25% | Time: 182.4s\nEpoch [S1_E2] VALID - Loss: 0.6448 | Top-1: 95.81%\nEpoch [3] Step [100/391] Loss: 0.6771 | Top-1: 94.57%\nEpoch [3] Step [200/391] Loss: 0.6750 | Top-1: 94.64%\nEpoch [3] Step [300/391] Loss: 0.6761 | Top-1: 94.55%\nEpoch [3] TRAIN - Loss: 0.6736 | Top-1: 94.65% | Time: 181.5s\nEpoch [S1_E3] VALID - Loss: 0.6387 | Top-1: 96.04%\nEpoch [4] Step [100/391] Loss: 0.6710 | Top-1: 94.66%\nEpoch [4] Step [200/391] Loss: 0.6691 | Top-1: 94.88%\nEpoch [4] Step [300/391] Loss: 0.6701 | Top-1: 94.84%\nEpoch [4] TRAIN - Loss: 0.6703 | Top-1: 94.85% | Time: 181.7s\nEpoch [S1_E4] VALID - Loss: 0.6376 | Top-1: 96.08%\nEnd of stage 1: best_val_acc=96.08%, sparsity=9.32%, compression=1.10x\nSaved stage 1 checkpoint to: mobilenetv2_cifar10_pruned_stage1.pth\n============================================================\nStage 2: target_global_prune = 0.15\n============================================================\nPer-layer prune fractions at this stage:\nfeatures.1.conv.1         -> 0.02\nfeatures.2.conv.0.0       -> 0.10\nfeatures.2.conv.2         -> 0.06\nfeatures.3.conv.0.0       -> 0.10\nfeatures.3.conv.2         -> 0.10\nfeatures.4.conv.0.0       -> 0.04\nfeatures.4.conv.2         -> 0.05\nfeatures.5.conv.0.0       -> 0.09\nfeatures.5.conv.2         -> 0.20\nfeatures.6.conv.0.0       -> 0.12\nfeatures.6.conv.2         -> 0.20\nfeatures.7.conv.0.0       -> 0.04\nfeatures.7.conv.2         -> 0.05\nfeatures.8.conv.0.0       -> 0.21\nfeatures.8.conv.2         -> 0.09\nfeatures.9.conv.0.0       -> 0.21\nfeatures.9.conv.2         -> 0.08\nfeatures.10.conv.0.0      -> 0.12\nfeatures.10.conv.2        -> 0.16\nfeatures.11.conv.0.0      -> 0.06\nfeatures.11.conv.2        -> 0.04\nfeatures.12.conv.0.0      -> 0.16\nfeatures.12.conv.2        -> 0.06\nfeatures.13.conv.0.0      -> 0.07\nfeatures.13.conv.2        -> 0.10\nfeatures.14.conv.0.0      -> 0.09\nfeatures.14.conv.2        -> 0.04\nfeatures.15.conv.0.0      -> 0.04\nfeatures.15.conv.2        -> 0.09\nfeatures.16.conv.0.0      -> 0.12\nfeatures.16.conv.2        -> 0.08\nfeatures.17.conv.0.0      -> 0.21\nfeatures.17.conv.2        -> 0.31\nfeatures.18.0             -> 0.20\nEpoch [PRUNED_RAW_S2] VALID - Loss: 0.8506 | Top-1: 90.20%\nStage 2 - accuracy after pruning, before fine-tune: 90.20%\nStage 2 - sparsity: 14.02% (compression 1.16x, nonzero 1923074/2236682)\nEpoch [5] Step [100/391] Loss: 0.7323 | Top-1: 92.84%\nEpoch [5] Step [200/391] Loss: 0.7199 | Top-1: 93.16%\nEpoch [5] Step [300/391] Loss: 0.7145 | Top-1: 93.25%\nEpoch [5] TRAIN - Loss: 0.7120 | Top-1: 93.29% | Time: 181.5s\nEpoch [S2_E0] VALID - Loss: 0.6536 | Top-1: 95.50%\nEpoch [6] Step [100/391] Loss: 0.6987 | Top-1: 93.62%\nEpoch [6] Step [200/391] Loss: 0.6961 | Top-1: 93.86%\nEpoch [6] Step [300/391] Loss: 0.6972 | Top-1: 93.85%\nEpoch [6] TRAIN - Loss: 0.6978 | Top-1: 93.78% | Time: 181.3s\nEpoch [S2_E1] VALID - Loss: 0.6499 | Top-1: 95.41%\nEpoch [7] Step [100/391] Loss: 0.6843 | Top-1: 94.16%\nEpoch [7] Step [200/391] Loss: 0.6835 | Top-1: 94.26%\nEpoch [7] Step [300/391] Loss: 0.6826 | Top-1: 94.29%\nEpoch [7] TRAIN - Loss: 0.6835 | Top-1: 94.23% | Time: 181.3s\nEpoch [S2_E2] VALID - Loss: 0.6439 | Top-1: 95.78%\nEpoch [8] Step [100/391] Loss: 0.6780 | Top-1: 94.69%\nEpoch [8] Step [200/391] Loss: 0.6771 | Top-1: 94.64%\nEpoch [8] Step [300/391] Loss: 0.6753 | Top-1: 94.65%\nEpoch [8] TRAIN - Loss: 0.6760 | Top-1: 94.62% | Time: 181.4s\nEpoch [S2_E3] VALID - Loss: 0.6394 | Top-1: 95.85%\nEpoch [9] Step [100/391] Loss: 0.6709 | Top-1: 94.70%\nEpoch [9] Step [200/391] Loss: 0.6691 | Top-1: 94.87%\nEpoch [9] Step [300/391] Loss: 0.6700 | Top-1: 94.89%\nEpoch [9] TRAIN - Loss: 0.6701 | Top-1: 94.83% | Time: 181.3s\nEpoch [S2_E4] VALID - Loss: 0.6379 | Top-1: 96.06%\nEnd of stage 2: best_val_acc=96.06%, sparsity=14.02%, compression=1.16x\nSaved stage 2 checkpoint to: mobilenetv2_cifar10_pruned_stage2.pth\n============================================================\nStage 3: target_global_prune = 0.20\n============================================================\nPer-layer prune fractions at this stage:\nfeatures.1.conv.1         -> 0.03\nfeatures.2.conv.0.0       -> 0.10\nfeatures.2.conv.2         -> 0.10\nfeatures.3.conv.0.0       -> 0.10\nfeatures.3.conv.2         -> 0.10\nfeatures.4.conv.0.0       -> 0.07\nfeatures.4.conv.2         -> 0.09\nfeatures.5.conv.0.0       -> 0.15\nfeatures.5.conv.2         -> 0.20\nfeatures.6.conv.0.0       -> 0.20\nfeatures.6.conv.2         -> 0.20\nfeatures.7.conv.0.0       -> 0.06\nfeatures.7.conv.2         -> 0.08\nfeatures.8.conv.0.0       -> 0.30\nfeatures.8.conv.2         -> 0.15\nfeatures.9.conv.0.0       -> 0.30\nfeatures.9.conv.2         -> 0.13\nfeatures.10.conv.0.0      -> 0.21\nfeatures.10.conv.2        -> 0.26\nfeatures.11.conv.0.0      -> 0.10\nfeatures.11.conv.2        -> 0.07\nfeatures.12.conv.0.0      -> 0.26\nfeatures.12.conv.2        -> 0.10\nfeatures.13.conv.0.0      -> 0.12\nfeatures.13.conv.2        -> 0.17\nfeatures.14.conv.0.0      -> 0.15\nfeatures.14.conv.2        -> 0.06\nfeatures.15.conv.0.0      -> 0.07\nfeatures.15.conv.2        -> 0.15\nfeatures.16.conv.0.0      -> 0.21\nfeatures.16.conv.2        -> 0.13\nfeatures.17.conv.0.0      -> 0.35\nfeatures.17.conv.2        -> 0.35\nfeatures.18.0             -> 0.20\nEpoch [PRUNED_RAW_S3] VALID - Loss: 1.5053 | Top-1: 75.33%\nStage 3 - accuracy after pruning, before fine-tune: 75.33%\nStage 3 - sparsity: 18.76% (compression 1.23x, nonzero 1817056/2236682)\nEpoch [10] Step [100/391] Loss: 0.7514 | Top-1: 91.85%\nEpoch [10] Step [200/391] Loss: 0.7311 | Top-1: 92.55%\nEpoch [10] Step [300/391] Loss: 0.7210 | Top-1: 92.85%\nEpoch [10] TRAIN - Loss: 0.7169 | Top-1: 93.02% | Time: 180.9s\nEpoch [S3_E0] VALID - Loss: 0.6611 | Top-1: 95.08%\nEpoch [11] Step [100/391] Loss: 0.6935 | Top-1: 93.85%\nEpoch [11] Step [200/391] Loss: 0.6963 | Top-1: 93.75%\nEpoch [11] Step [300/391] Loss: 0.6941 | Top-1: 93.86%\nEpoch [11] TRAIN - Loss: 0.6962 | Top-1: 93.76% | Time: 180.8s\nEpoch [S3_E1] VALID - Loss: 0.6565 | Top-1: 95.29%\nEpoch [12] Step [100/391] Loss: 0.6861 | Top-1: 94.28%\nEpoch [12] Step [200/391] Loss: 0.6878 | Top-1: 94.18%\nEpoch [12] Step [300/391] Loss: 0.6867 | Top-1: 94.17%\nEpoch [12] TRAIN - Loss: 0.6873 | Top-1: 94.13% | Time: 181.0s\nEpoch [S3_E2] VALID - Loss: 0.6521 | Top-1: 95.31%\nEpoch [13] Step [100/391] Loss: 0.6793 | Top-1: 94.65%\nEpoch [13] Step [200/391] Loss: 0.6819 | Top-1: 94.42%\nEpoch [13] Step [300/391] Loss: 0.6807 | Top-1: 94.47%\nEpoch [13] TRAIN - Loss: 0.6798 | Top-1: 94.48% | Time: 181.2s\nEpoch [S3_E3] VALID - Loss: 0.6450 | Top-1: 95.64%\nEpoch [14] Step [100/391] Loss: 0.6706 | Top-1: 94.66%\nEpoch [14] Step [200/391] Loss: 0.6723 | Top-1: 94.70%\nEpoch [14] Step [300/391] Loss: 0.6736 | Top-1: 94.66%\nEpoch [14] TRAIN - Loss: 0.6735 | Top-1: 94.74% | Time: 181.1s\nEpoch [S3_E4] VALID - Loss: 0.6394 | Top-1: 96.00%\nEnd of stage 3: best_val_acc=96.00%, sparsity=18.76%, compression=1.23x\nSaved stage 3 checkpoint to: mobilenetv2_cifar10_pruned_stage3.pth\n============================================================\nStage 4: target_global_prune = 0.25\n============================================================\nPer-layer prune fractions at this stage:\nfeatures.1.conv.1         -> 0.05\nfeatures.2.conv.0.0       -> 0.10\nfeatures.2.conv.2         -> 0.10\nfeatures.3.conv.0.0       -> 0.10\nfeatures.3.conv.2         -> 0.10\nfeatures.4.conv.0.0       -> 0.11\nfeatures.4.conv.2         -> 0.15\nfeatures.5.conv.0.0       -> 0.20\nfeatures.5.conv.2         -> 0.20\nfeatures.6.conv.0.0       -> 0.20\nfeatures.6.conv.2         -> 0.20\nfeatures.7.conv.0.0       -> 0.11\nfeatures.7.conv.2         -> 0.14\nfeatures.8.conv.0.0       -> 0.30\nfeatures.8.conv.2         -> 0.26\nfeatures.9.conv.0.0       -> 0.30\nfeatures.9.conv.2         -> 0.23\nfeatures.10.conv.0.0      -> 0.30\nfeatures.10.conv.2        -> 0.30\nfeatures.11.conv.0.0      -> 0.17\nfeatures.11.conv.2        -> 0.11\nfeatures.12.conv.0.0      -> 0.30\nfeatures.12.conv.2        -> 0.18\nfeatures.13.conv.0.0      -> 0.20\nfeatures.13.conv.2        -> 0.30\nfeatures.14.conv.0.0      -> 0.26\nfeatures.14.conv.2        -> 0.11\nfeatures.15.conv.0.0      -> 0.13\nfeatures.15.conv.2        -> 0.26\nfeatures.16.conv.0.0      -> 0.35\nfeatures.16.conv.2        -> 0.23\nfeatures.17.conv.0.0      -> 0.35\nfeatures.17.conv.2        -> 0.35\nfeatures.18.0             -> 0.20\nEpoch [PRUNED_RAW_S4] VALID - Loss: 1.7724 | Top-1: 44.46%\nStage 4 - accuracy after pruning, before fine-tune: 44.46%\nStage 4 - sparsity: 23.52% (compression 1.31x, nonzero 1710698/2236682)\nEpoch [15] Step [100/391] Loss: 0.8179 | Top-1: 88.62%\nEpoch [15] Step [200/391] Loss: 0.7882 | Top-1: 89.73%\nEpoch [15] Step [300/391] Loss: 0.7746 | Top-1: 90.25%\nEpoch [15] TRAIN - Loss: 0.7689 | Top-1: 90.50% | Time: 180.9s\nEpoch [S4_E0] VALID - Loss: 0.6689 | Top-1: 94.80%\nEpoch [16] Step [100/391] Loss: 0.7262 | Top-1: 92.40%\nEpoch [16] Step [200/391] Loss: 0.7242 | Top-1: 92.36%\nEpoch [16] Step [300/391] Loss: 0.7241 | Top-1: 92.41%\nEpoch [16] TRAIN - Loss: 0.7235 | Top-1: 92.44% | Time: 180.9s\nEpoch [S4_E1] VALID - Loss: 0.6649 | Top-1: 94.74%\nEpoch [17] Step [100/391] Loss: 0.7092 | Top-1: 93.23%\nEpoch [17] Step [200/391] Loss: 0.7100 | Top-1: 93.06%\nEpoch [17] Step [300/391] Loss: 0.7094 | Top-1: 93.05%\nEpoch [17] TRAIN - Loss: 0.7089 | Top-1: 93.11% | Time: 180.8s\nEpoch [S4_E2] VALID - Loss: 0.6544 | Top-1: 95.36%\nEpoch [18] Step [100/391] Loss: 0.7015 | Top-1: 93.55%\nEpoch [18] Step [200/391] Loss: 0.6998 | Top-1: 93.53%\nEpoch [18] Step [300/391] Loss: 0.6962 | Top-1: 93.72%\nEpoch [18] TRAIN - Loss: 0.6951 | Top-1: 93.80% | Time: 181.1s\nEpoch [S4_E3] VALID - Loss: 0.6473 | Top-1: 95.49%\nEpoch [19] Step [100/391] Loss: 0.6939 | Top-1: 93.80%\nEpoch [19] Step [200/391] Loss: 0.6865 | Top-1: 94.16%\nEpoch [19] Step [300/391] Loss: 0.6873 | Top-1: 94.11%\nEpoch [19] TRAIN - Loss: 0.6861 | Top-1: 94.20% | Time: 181.0s\nEpoch [S4_E4] VALID - Loss: 0.6454 | Top-1: 95.73%\nEnd of stage 4: best_val_acc=95.73%, sparsity=23.52%, compression=1.31x\nSaved stage 4 checkpoint to: mobilenetv2_cifar10_pruned_stage4.pth\n============================================================\nStage 5: target_global_prune = 0.30\n============================================================\nPer-layer prune fractions at this stage:\nfeatures.1.conv.1         -> 0.10\nfeatures.2.conv.0.0       -> 0.10\nfeatures.2.conv.2         -> 0.10\nfeatures.3.conv.0.0       -> 0.10\nfeatures.3.conv.2         -> 0.10\nfeatures.4.conv.0.0       -> 0.20\nfeatures.4.conv.2         -> 0.20\nfeatures.5.conv.0.0       -> 0.20\nfeatures.5.conv.2         -> 0.20\nfeatures.6.conv.0.0       -> 0.20\nfeatures.6.conv.2         -> 0.20\nfeatures.7.conv.0.0       -> 0.20\nfeatures.7.conv.2         -> 0.20\nfeatures.8.conv.0.0       -> 0.30\nfeatures.8.conv.2         -> 0.30\nfeatures.9.conv.0.0       -> 0.30\nfeatures.9.conv.2         -> 0.30\nfeatures.10.conv.0.0      -> 0.30\nfeatures.10.conv.2        -> 0.30\nfeatures.11.conv.0.0      -> 0.30\nfeatures.11.conv.2        -> 0.28\nfeatures.12.conv.0.0      -> 0.30\nfeatures.12.conv.2        -> 0.30\nfeatures.13.conv.0.0      -> 0.30\nfeatures.13.conv.2        -> 0.30\nfeatures.14.conv.0.0      -> 0.35\nfeatures.14.conv.2        -> 0.27\nfeatures.15.conv.0.0      -> 0.32\nfeatures.15.conv.2        -> 0.35\nfeatures.16.conv.0.0      -> 0.35\nfeatures.16.conv.2        -> 0.35\nfeatures.17.conv.0.0      -> 0.35\nfeatures.17.conv.2        -> 0.35\nfeatures.18.0             -> 0.20\nEpoch [PRUNED_RAW_S5] VALID - Loss: 2.2925 | Top-1: 23.14%\nStage 5 - accuracy after pruning, before fine-tune: 23.14%\nStage 5 - sparsity: 28.26% (compression 1.39x, nonzero 1604682/2236682)\nEpoch [20] Step [100/391] Loss: 0.9059 | Top-1: 84.63%\nEpoch [20] Step [200/391] Loss: 0.8576 | Top-1: 86.60%\nEpoch [20] Step [300/391] Loss: 0.8301 | Top-1: 87.68%\nEpoch [20] TRAIN - Loss: 0.8176 | Top-1: 88.19% | Time: 180.5s\nEpoch [S5_E0] VALID - Loss: 0.6866 | Top-1: 93.71%\nEpoch [21] Step [100/391] Loss: 0.7550 | Top-1: 91.09%\nEpoch [21] Step [200/391] Loss: 0.7535 | Top-1: 91.12%\nEpoch [21] Step [300/391] Loss: 0.7514 | Top-1: 91.17%\nEpoch [21] TRAIN - Loss: 0.7517 | Top-1: 91.11% | Time: 180.6s\nEpoch [S5_E1] VALID - Loss: 0.6738 | Top-1: 94.44%\nEpoch [22] Step [100/391] Loss: 0.7359 | Top-1: 91.88%\nEpoch [22] Step [200/391] Loss: 0.7359 | Top-1: 91.84%\nEpoch [22] Step [300/391] Loss: 0.7345 | Top-1: 91.93%\nEpoch [22] TRAIN - Loss: 0.7326 | Top-1: 92.00% | Time: 180.5s\nEpoch [S5_E2] VALID - Loss: 0.6606 | Top-1: 94.98%\nEpoch [23] Step [100/391] Loss: 0.7234 | Top-1: 92.40%\nEpoch [23] Step [200/391] Loss: 0.7210 | Top-1: 92.48%\nEpoch [23] Step [300/391] Loss: 0.7180 | Top-1: 92.71%\nEpoch [23] TRAIN - Loss: 0.7147 | Top-1: 92.82% | Time: 180.6s\nEpoch [S5_E3] VALID - Loss: 0.6566 | Top-1: 95.22%\nEpoch [24] Step [100/391] Loss: 0.7026 | Top-1: 93.52%\nEpoch [24] Step [200/391] Loss: 0.7028 | Top-1: 93.46%\nEpoch [24] Step [300/391] Loss: 0.7009 | Top-1: 93.46%\nEpoch [24] TRAIN - Loss: 0.7010 | Top-1: 93.47% | Time: 180.7s\nEpoch [S5_E4] VALID - Loss: 0.6526 | Top-1: 95.11%\nEnd of stage 5: best_val_acc=95.22%, sparsity=28.26%, compression=1.39x\nSaved stage 5 checkpoint to: mobilenetv2_cifar10_pruned_stage5.pth\n\n=== Compression vs Accuracy per Stage ===\nStage 1: target=0.10, sparsity=9.32%, compression=1.10x, acc_after_prune=91.98%, best_val_acc=96.08%, ckpt=mobilenetv2_cifar10_pruned_stage1.pth\nStage 2: target=0.15, sparsity=14.02%, compression=1.16x, acc_after_prune=90.20%, best_val_acc=96.06%, ckpt=mobilenetv2_cifar10_pruned_stage2.pth\nStage 3: target=0.20, sparsity=18.76%, compression=1.23x, acc_after_prune=75.33%, best_val_acc=96.00%, ckpt=mobilenetv2_cifar10_pruned_stage3.pth\nStage 4: target=0.25, sparsity=23.52%, compression=1.31x, acc_after_prune=44.46%, best_val_acc=95.73%, ckpt=mobilenetv2_cifar10_pruned_stage4.pth\nStage 5: target=0.30, sparsity=28.26%, compression=1.39x, acc_after_prune=23.14%, best_val_acc=95.22%, ckpt=mobilenetv2_cifar10_pruned_stage5.pth\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- Final fine-tuning of best pruned model before quantization ---\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Rebuild the same architecture\nmodel = create_mobilenetv2_cifar10(num_classes=10, pretrained=False)\n\nckpt = torch.load(\"mobilenetv2_cifar10_pruned_stage5.pth\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"state_dict\"], strict=True)\nmodel = model.to(device)\n\n# Recreate data loaders if needed\ntrainloader, testloader = get_cifar10_loaders(\n    cfg.data_dir,\n    batch_size=cfg.batch_size,\n    num_workers=cfg.num_workers,\n)\n\ncriterion = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\n\nfine_tune_epochs = 15       # 5–20 is typical\nfine_tune_lr     = 0.005    # if original was 0.05\n\noptimizer, scheduler = build_optimizer_and_scheduler(\n    model,\n    base_lr=fine_tune_lr,\n    weight_decay=cfg.weight_decay,\n    stage_epochs=fine_tune_epochs,\n)\n\nbest_ft_acc = 0.0\nbest_ft_state = None\n\nfor epoch in range(fine_tune_epochs):\n    train_loss, train_acc = train_one_epoch(\n        model, criterion, optimizer, trainloader, device, epoch\n    )\n    val_loss, val_acc = evaluate(\n        model, criterion, testloader, device, epoch=f\"FT_{epoch}\"\n    )\n\n    scheduler.step()\n\n    if val_acc > best_ft_acc:\n        best_ft_acc = val_acc\n        best_ft_state = {k: v.cpu() for k, v in model.state_dict().items()}\n        print(f\"[FT] New best accuracy: {best_ft_acc:.2f}% at epoch {epoch}\")\n\nif best_ft_state is not None:\n    torch.save(best_ft_state, \"mobilenetv2_cifar10_pruned_finetuned.pth\")\n    print(f\"Saved final pruned+finetuned model with acc={best_ft_acc:.2f}%\")\nelse:\n    # fallback if somehow no improvement happened\n    torch.save(model.state_dict(), \"mobilenetv2_cifar10_pruned_finetuned.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T17:27:12.326373Z","iopub.execute_input":"2025-12-13T17:27:12.326906Z","iopub.status.idle":"2025-12-13T18:15:16.038532Z","shell.execute_reply.started":"2025-12-13T17:27:12.326882Z","shell.execute_reply":"2025-12-13T18:15:16.037802Z"}},"outputs":[{"name":"stdout","text":"Epoch [0] Step [100/391] Loss: 0.7011 | Top-1: 93.66%\nEpoch [0] Step [200/391] Loss: 0.7015 | Top-1: 93.43%\nEpoch [0] Step [300/391] Loss: 0.7036 | Top-1: 93.26%\nEpoch [0] TRAIN - Loss: 0.7044 | Top-1: 93.25% | Time: 174.5s\nEpoch [FT_0] VALID - Loss: 0.6589 | Top-1: 95.09%\n[FT] New best accuracy: 95.09% at epoch 0\nEpoch [1] Step [100/391] Loss: 0.7079 | Top-1: 93.16%\nEpoch [1] Step [200/391] Loss: 0.7061 | Top-1: 93.19%\nEpoch [1] Step [300/391] Loss: 0.7053 | Top-1: 93.16%\nEpoch [1] TRAIN - Loss: 0.7053 | Top-1: 93.19% | Time: 180.1s\nEpoch [FT_1] VALID - Loss: 0.6573 | Top-1: 95.14%\n[FT] New best accuracy: 95.14% at epoch 1\nEpoch [2] Step [100/391] Loss: 0.7005 | Top-1: 93.66%\nEpoch [2] Step [200/391] Loss: 0.6950 | Top-1: 93.78%\nEpoch [2] Step [300/391] Loss: 0.6981 | Top-1: 93.62%\nEpoch [2] TRAIN - Loss: 0.6995 | Top-1: 93.58% | Time: 181.9s\nEpoch [FT_2] VALID - Loss: 0.6554 | Top-1: 95.14%\nEpoch [3] Step [100/391] Loss: 0.7002 | Top-1: 93.48%\nEpoch [3] Step [200/391] Loss: 0.6982 | Top-1: 93.55%\nEpoch [3] Step [300/391] Loss: 0.6988 | Top-1: 93.50%\nEpoch [3] TRAIN - Loss: 0.6978 | Top-1: 93.58% | Time: 181.6s\nEpoch [FT_3] VALID - Loss: 0.6558 | Top-1: 95.09%\nEpoch [4] Step [100/391] Loss: 0.6990 | Top-1: 93.52%\nEpoch [4] Step [200/391] Loss: 0.6958 | Top-1: 93.63%\nEpoch [4] Step [300/391] Loss: 0.6942 | Top-1: 93.70%\nEpoch [4] TRAIN - Loss: 0.6939 | Top-1: 93.69% | Time: 181.9s\nEpoch [FT_4] VALID - Loss: 0.6534 | Top-1: 95.32%\n[FT] New best accuracy: 95.32% at epoch 4\nEpoch [5] Step [100/391] Loss: 0.6948 | Top-1: 93.56%\nEpoch [5] Step [200/391] Loss: 0.6928 | Top-1: 93.71%\nEpoch [5] Step [300/391] Loss: 0.6893 | Top-1: 93.90%\nEpoch [5] TRAIN - Loss: 0.6903 | Top-1: 93.87% | Time: 182.1s\nEpoch [FT_5] VALID - Loss: 0.6516 | Top-1: 95.37%\n[FT] New best accuracy: 95.37% at epoch 5\nEpoch [6] Step [100/391] Loss: 0.6868 | Top-1: 94.05%\nEpoch [6] Step [200/391] Loss: 0.6876 | Top-1: 94.04%\nEpoch [6] Step [300/391] Loss: 0.6872 | Top-1: 93.97%\nEpoch [6] TRAIN - Loss: 0.6879 | Top-1: 93.98% | Time: 182.1s\nEpoch [FT_6] VALID - Loss: 0.6508 | Top-1: 95.35%\nEpoch [7] Step [100/391] Loss: 0.6848 | Top-1: 94.21%\nEpoch [7] Step [200/391] Loss: 0.6862 | Top-1: 94.05%\nEpoch [7] Step [300/391] Loss: 0.6848 | Top-1: 94.17%\nEpoch [7] TRAIN - Loss: 0.6846 | Top-1: 94.18% | Time: 181.8s\nEpoch [FT_7] VALID - Loss: 0.6488 | Top-1: 95.43%\n[FT] New best accuracy: 95.43% at epoch 7\nEpoch [8] Step [100/391] Loss: 0.6853 | Top-1: 94.09%\nEpoch [8] Step [200/391] Loss: 0.6832 | Top-1: 94.26%\nEpoch [8] Step [300/391] Loss: 0.6825 | Top-1: 94.33%\nEpoch [8] TRAIN - Loss: 0.6814 | Top-1: 94.40% | Time: 182.1s\nEpoch [FT_8] VALID - Loss: 0.6466 | Top-1: 95.56%\n[FT] New best accuracy: 95.56% at epoch 8\nEpoch [9] Step [100/391] Loss: 0.6767 | Top-1: 94.53%\nEpoch [9] Step [200/391] Loss: 0.6781 | Top-1: 94.36%\nEpoch [9] Step [300/391] Loss: 0.6764 | Top-1: 94.49%\nEpoch [9] TRAIN - Loss: 0.6759 | Top-1: 94.49% | Time: 181.8s\nEpoch [FT_9] VALID - Loss: 0.6453 | Top-1: 95.57%\n[FT] New best accuracy: 95.57% at epoch 9\nEpoch [10] Step [100/391] Loss: 0.6818 | Top-1: 94.44%\nEpoch [10] Step [200/391] Loss: 0.6787 | Top-1: 94.52%\nEpoch [10] Step [300/391] Loss: 0.6777 | Top-1: 94.54%\nEpoch [10] TRAIN - Loss: 0.6775 | Top-1: 94.54% | Time: 181.9s\nEpoch [FT_10] VALID - Loss: 0.6457 | Top-1: 95.64%\n[FT] New best accuracy: 95.64% at epoch 10\nEpoch [11] Step [100/391] Loss: 0.6724 | Top-1: 94.62%\nEpoch [11] Step [200/391] Loss: 0.6745 | Top-1: 94.55%\nEpoch [11] Step [300/391] Loss: 0.6742 | Top-1: 94.54%\nEpoch [11] TRAIN - Loss: 0.6736 | Top-1: 94.59% | Time: 182.1s\nEpoch [FT_11] VALID - Loss: 0.6441 | Top-1: 95.62%\nEpoch [12] Step [100/391] Loss: 0.6673 | Top-1: 94.84%\nEpoch [12] Step [200/391] Loss: 0.6698 | Top-1: 94.88%\nEpoch [12] Step [300/391] Loss: 0.6700 | Top-1: 94.90%\nEpoch [12] TRAIN - Loss: 0.6705 | Top-1: 94.86% | Time: 182.3s\nEpoch [FT_12] VALID - Loss: 0.6437 | Top-1: 95.59%\nEpoch [13] Step [100/391] Loss: 0.6678 | Top-1: 95.05%\nEpoch [13] Step [200/391] Loss: 0.6669 | Top-1: 95.08%\nEpoch [13] Step [300/391] Loss: 0.6675 | Top-1: 95.05%\nEpoch [13] TRAIN - Loss: 0.6680 | Top-1: 94.98% | Time: 182.3s\nEpoch [FT_13] VALID - Loss: 0.6428 | Top-1: 95.73%\n[FT] New best accuracy: 95.73% at epoch 13\nEpoch [14] Step [100/391] Loss: 0.6695 | Top-1: 95.09%\nEpoch [14] Step [200/391] Loss: 0.6691 | Top-1: 94.97%\nEpoch [14] Step [300/391] Loss: 0.6698 | Top-1: 94.89%\nEpoch [14] TRAIN - Loss: 0.6696 | Top-1: 94.89% | Time: 182.1s\nEpoch [FT_14] VALID - Loss: 0.6433 | Top-1: 95.73%\nSaved final pruned+finetuned model with acc=95.73%\n","output_type":"stream"}],"execution_count":23}]}