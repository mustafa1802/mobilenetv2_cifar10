{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Baseline MobileNetV2 on CIFAR-10 dataset","metadata":{}},{"cell_type":"code","source":"import argparse\nimport os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torchvision.models import mobilenet_v2, MobileNet_V2_Weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.554357Z","iopub.execute_input":"2025-12-14T09:41:39.554747Z","iopub.status.idle":"2025-12-14T09:41:39.559493Z","shell.execute_reply.started":"2025-12-14T09:41:39.554714Z","shell.execute_reply":"2025-12-14T09:41:39.558501Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_mobilenetv2_cifar10(num_classes=10, pretrained=True):\n    \"\"\"\n    Create a MobileNetV2 model adapted for CIFAR-10.\n    \"\"\"\n    if pretrained:\n        print(\"Load ImageNet weights\")\n        weights = MobileNet_V2_Weights.IMAGENET1K_V1\n        model = mobilenet_v2(weights=weights)\n    else:\n        model = mobilenet_v2(weights=None)\n\n    # Replace classifier (last linear layer) for CIFAR-10\n    in_features = model.classifier[1].in_features\n    model.classifier[1] = nn.Linear(in_features, num_classes)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.560789Z","iopub.execute_input":"2025-12-14T09:41:39.561009Z","iopub.status.idle":"2025-12-14T09:41:39.581291Z","shell.execute_reply.started":"2025-12-14T09:41:39.560994Z","shell.execute_reply":"2025-12-14T09:41:39.580801Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Config:\n    data_dir = \"./data\"           # For Colab, use \"/content/data\"\n    epochs = 30\n    batch_size = 128\n    lr = 0.05\n    weight_decay = 4e-5\n    num_workers = 4\n    label_smoothing = 0.1\n    no_pretrained = False         # Set True to disable ImageNet pretraining\n    save_path = \"mobilenetv2_cifar10_best.pth\"\n    resume = \"\"                   # Path to checkpoint, or \"\" to start fresh\n    mixed_precision = False        # Use AMP if GPU is available\n\ncfg = Config()\nprint(\"Config:\", vars(cfg))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.582085Z","iopub.execute_input":"2025-12-14T09:41:39.582428Z","iopub.status.idle":"2025-12-14T09:41:39.599945Z","shell.execute_reply.started":"2025-12-14T09:41:39.582403Z","shell.execute_reply":"2025-12-14T09:41:39.599215Z"}},"outputs":[{"name":"stdout","text":"Config: {}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"PRUNE_FRACTIONS = {\n    \"features.1.conv.1\":   0.10,\n    \"features.2.conv.0.0\": 0.10,\n    \"features.2.conv.2\":   0.10,\n    \"features.3.conv.0.0\": 0.10,\n    \"features.3.conv.2\":   0.10,\n    \"features.4.conv.0.0\": 0.20,\n    \"features.4.conv.2\":   0.20,\n    \"features.5.conv.0.0\": 0.20,\n    \"features.5.conv.2\":   0.20,\n    \"features.6.conv.0.0\": 0.20,\n    \"features.6.conv.2\":   0.20,\n    \"features.7.conv.0.0\": 0.20,\n    \"features.7.conv.2\":   0.20,\n    \"features.8.conv.0.0\": 0.30,\n    \"features.8.conv.2\":   0.30,\n    \"features.9.conv.0.0\": 0.30,\n    \"features.9.conv.2\":   0.30,\n    \"features.10.conv.0.0\": 0.30,\n    \"features.10.conv.2\":   0.30,\n    \"features.11.conv.0.0\": 0.30,\n    \"features.11.conv.2\":   0.28,\n    \"features.12.conv.0.0\": 0.30,\n    \"features.12.conv.2\":   0.30,\n    \"features.13.conv.0.0\": 0.30,\n    \"features.13.conv.2\":   0.30,\n    \"features.14.conv.0.0\": 0.35,\n    \"features.14.conv.2\":   0.27,\n    \"features.15.conv.0.0\": 0.32,\n    \"features.15.conv.2\":   0.35,\n    \"features.16.conv.0.0\": 0.35,\n    \"features.16.conv.2\":   0.35,\n    \"features.17.conv.0.0\": 0.35,\n    \"features.17.conv.2\":   0.35,\n    \"features.18.0\":        0.20,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.600757Z","iopub.execute_input":"2025-12-14T09:41:39.601002Z","iopub.status.idle":"2025-12-14T09:41:39.619248Z","shell.execute_reply.started":"2025-12-14T09:41:39.600986Z","shell.execute_reply":"2025-12-14T09:41:39.618731Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def collect_activation_sizes_baseline(model: nn.Module,\n                                      input_tensor: torch.Tensor):\n    \"\"\"\n    Runs a forward pass on the *baseline* (unpruned) model and\n    returns a dict: {module_name: num_elements_in_output_activation}.\n    \"\"\"\n    act_sizes = {}\n    handles = []\n\n    def make_hook(name):\n        def hook(module, inp, out):\n            if isinstance(out, torch.Tensor):\n                act_sizes[name] = out.numel()\n            elif isinstance(out, (tuple, list)):\n                act_sizes[name] = sum(\n                    o.numel() for o in out if isinstance(o, torch.Tensor)\n                )\n        return hook\n\n    # Attach hooks to conv/linear layers\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.Conv2d, nn.Linear)):\n            h = module.register_forward_hook(make_hook(name))\n            handles.append(h)\n\n    model.eval()\n    with torch.no_grad():\n        _ = model(input_tensor)\n\n    # Clean up\n    for h in handles:\n        h.remove()\n\n    return act_sizes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.620610Z","iopub.execute_input":"2025-12-14T09:41:39.620964Z","iopub.status.idle":"2025-12-14T09:41:39.636266Z","shell.execute_reply.started":"2025-12-14T09:41:39.620942Z","shell.execute_reply":"2025-12-14T09:41:39.635732Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def compute_runtime_activation_compression_with_table(\n    baseline_model: nn.Module,\n    dummy_input: torch.Tensor,\n    prune_fractions: dict,\n    activation_bit_width: int = 8,\n):\n    \"\"\"\n    Computes:\n      - overall runtime activation compression vs baseline FP32\n      - per-layer table with prune fraction and per-layer compression\n\n    Returns:\n      compression_ratio (float),\n      layer_rows (list of dicts)\n    \"\"\"\n    # 1) Collect baseline activation sizes\n    act_sizes = collect_activation_sizes_baseline(baseline_model, dummy_input)\n\n    bits_baseline_total = 0.0\n    bits_pruned_quant_total = 0.0\n    layer_rows = []\n\n    for name, N in act_sizes.items():\n        # Baseline bits (FP32)\n        bits_baseline = N * 32\n\n        # Prune fraction (0.0 if not pruned / not in dict)\n        p_l = prune_fractions.get(name, 0.0)\n\n        # Pruned + quantized bits\n        bits_pruned_quant = (1.0 - p_l) * N * activation_bit_width\n\n        bits_baseline_total += bits_baseline\n        bits_pruned_quant_total += bits_pruned_quant\n\n        # Per-layer compression: baseline FP32 -> pruned + quantized\n        if bits_pruned_quant > 0:\n            layer_compression = bits_baseline / bits_pruned_quant\n        else:\n            layer_compression = float(\"inf\")\n\n        layer_rows.append({\n            \"layer\": name,\n            \"N_elements\": N,\n            \"prune_fraction\": p_l,\n            \"baseline_bits\": bits_baseline,\n            \"pruned_quant_bits\": bits_pruned_quant,\n            \"compression\": layer_compression,\n        })\n\n    overall_compression = bits_baseline_total / max(bits_pruned_quant_total, 1e-8)\n    return overall_compression, layer_rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.636941Z","iopub.execute_input":"2025-12-14T09:41:39.637107Z","iopub.status.idle":"2025-12-14T09:41:39.657749Z","shell.execute_reply.started":"2025-12-14T09:41:39.637093Z","shell.execute_reply":"2025-12-14T09:41:39.657182Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 1) Build your *baseline* (unpruned) MobileNetV2 for CIFAR-10\n# Model\nbaseline_model = create_mobilenetv2_cifar10(\n    num_classes=10,\n    pretrained=not cfg.no_pretrained,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbaseline_model = baseline_model.to(device)\n\n# 2) Representative input\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\n\n# 3) Compute overall compression + per-layer stats\noverall_act_compression, layer_rows = compute_runtime_activation_compression_with_table(\n    baseline_model=baseline_model,\n    dummy_input=dummy_input,\n    prune_fractions=PRUNE_FRACTIONS,\n    activation_bit_width=8,\n)\n\nprint(f\"\\nOverall runtime activation compression vs baseline FP32: \"\n      f\"{overall_act_compression:.2f}x\\n\")\n\n# 4) Print a small per-layer table\nprint(f\"{'Layer':40s} {'Prune':>7s} {'Comp(x)':>8s}\")\nprint(\"-\" * 60)\nfor row in layer_rows:\n    name = row[\"layer\"]\n    # Only print layers that are in your prune dict (or print all if you prefer)\n    if name in PRUNE_FRACTIONS:\n        p_l = row[\"prune_fraction\"]\n        comp = row[\"compression\"]\n        print(f\"{name:40s} {p_l:7.2f} {comp:8.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:41:39.705137Z","iopub.execute_input":"2025-12-14T09:41:39.705335Z","iopub.status.idle":"2025-12-14T09:41:39.827290Z","shell.execute_reply.started":"2025-12-14T09:41:39.705319Z","shell.execute_reply":"2025-12-14T09:41:39.826698Z"}},"outputs":[{"name":"stdout","text":"Load ImageNet weights\n\nOverall runtime activation compression vs baseline FP32: 4.47x\n\nLayer                                      Prune  Comp(x)\n------------------------------------------------------------\nfeatures.1.conv.1                           0.10     4.44\nfeatures.2.conv.0.0                         0.10     4.44\nfeatures.2.conv.2                           0.10     4.44\nfeatures.3.conv.0.0                         0.10     4.44\nfeatures.3.conv.2                           0.10     4.44\nfeatures.4.conv.0.0                         0.20     5.00\nfeatures.4.conv.2                           0.20     5.00\nfeatures.5.conv.0.0                         0.20     5.00\nfeatures.5.conv.2                           0.20     5.00\nfeatures.6.conv.0.0                         0.20     5.00\nfeatures.6.conv.2                           0.20     5.00\nfeatures.7.conv.0.0                         0.20     5.00\nfeatures.7.conv.2                           0.20     5.00\nfeatures.8.conv.0.0                         0.30     5.71\nfeatures.8.conv.2                           0.30     5.71\nfeatures.9.conv.0.0                         0.30     5.71\nfeatures.9.conv.2                           0.30     5.71\nfeatures.10.conv.0.0                        0.30     5.71\nfeatures.10.conv.2                          0.30     5.71\nfeatures.11.conv.0.0                        0.30     5.71\nfeatures.11.conv.2                          0.28     5.56\nfeatures.12.conv.0.0                        0.30     5.71\nfeatures.12.conv.2                          0.30     5.71\nfeatures.13.conv.0.0                        0.30     5.71\nfeatures.13.conv.2                          0.30     5.71\nfeatures.14.conv.0.0                        0.35     6.15\nfeatures.14.conv.2                          0.27     5.48\nfeatures.15.conv.0.0                        0.32     5.88\nfeatures.15.conv.2                          0.35     6.15\nfeatures.16.conv.0.0                        0.35     6.15\nfeatures.16.conv.2                          0.35     6.15\nfeatures.17.conv.0.0                        0.35     6.15\nfeatures.17.conv.2                          0.35     6.15\nfeatures.18.0                               0.20     5.00\n","output_type":"stream"}],"execution_count":14}]}